import numpy as np
import torch.nn.functional as F
from torch.utils.data import Dataset
from dateutil.relativedelta import relativedelta
import datetime
import xarray as xr


def cal_time_length(start_time, end_time):
    """
    Calculate the number of days between two dates.

    Args:
    - start_time (str): Start date in the format YYYYMMDD.
    - end_time (str): End date in the format YYYYMMDD.

    Returns:
    - int: Number of days between the two dates.
    """
    assert len(str(start_time)) == 6 & len(str(end_time)) == 6
    start = datetime.datetime.strptime(str(start_time), "%Y%m")
    end = datetime.datetime.strptime(str(end_time), "%Y%m")

    # 使用 relativedelta 计算月份差距
    delta = relativedelta(end, start)

    # 计算月份数差距并加上 1（包括开始日期）
    length = delta.years * 12 + delta.months + 1

    return length


def GenTimeList(start_time, end_time):
    """
    生成日期列表
    Args:
        start_time: 开始日期，格式为YYYYMMDD
        end_time: 结束日期，格式为YYYYMMDD
    Returns:
        日期列表，格式为list[YYYYMMDD]
    """
    Times = []
    current = datetime.datetime.strptime(str(start_time), "%Y%m")
    end = datetime.datetime.strptime(str(end_time), "%Y%m")

    while current <= end:
        Times.append(current.strftime("%Y%m"))
        current += relativedelta(months=1)

    return [int(x) for x in Times]


def read_process_data(txt, start_time, end_time):
    """
    读取和处理所有数据
    Args:
        start_time/end_time: YYYYMMDD
        txt: 包含要读取为图像的文件名列表的 .txt 文件
    Returns:
        datas: 经过填充处理和陆地标记的 SIC 数据
    """
    assert len(str(start_time)) == 6 and len(str(end_time)) == 6
    with open(txt, "r") as f:
        data_paths = np.array(f.read().split())

    times = np.array([int(x.split("_")[5][0:6]) for x in data_paths])
    data_paths = data_paths[(times >= start_time) & (times <= end_time)]

    datas = []
    for data_path in data_paths:
        data = xr.open_dataset(data_path)
        data = post_process_data(data)
        datas.append(data)
    assert len(datas) == cal_time_length(start_time, end_time)
    return datas


def write_netcdf(data_paths_file, start_time, end_time, out_path):
    """
    将处理过的数据写入 .nc 文件以便下次读取
    Args:
        data_paths_file: 包含要处理的所有数据的文件路径的文本文件的路径，这个文本文件可以通过'gen_data_text.sh'脚本生成，前提是文件都按照规则存放好，可以使用'data/move_dataset.py'对文件进行排列
        start_time/end_time: 数据的时间范围（YYYYMMDD格式，如20080101）
        out_path: 输出的 .nc 文件的路径
    Returns:
        ds: 写入的 xarray.Dataset 对象
    """
    # 读取并处理数据
    imgs = read_process_data(data_paths_file, start_time, end_time)
    # 创建 xarray 数据集并写入 .nc 文件
    time_coords = GenTimeList(start_time, end_time)
    x_coords = range(432)
    y_coords = range(432)

    ds = xr.Dataset(
        {"imgs": (["time", "yc", "xc"], imgs)},
        coords={"time": time_coords, "yc": y_coords, "xc": x_coords},
    )

    ds.to_netcdf(out_path)
    return ds


def post_process_data(data):
    """
    0 - 100 Sea ice concentration %
    -32767 Land

    处理数据，包括归一化、处理缺失数据、陆地屏蔽等
    Args:
        data: 输入的海冰数据
    Returns:
        ice_conc: 处理后的海冰密集度数据
    """
    ice_conc = np.array(data["ice_conc"][:][0])
    ice_conc = np.nan_to_num(ice_conc, nan=0)

    # 处理陆地
    ice_conc[ice_conc == -32767] = 0

    # 归一化
    ice_conc = ice_conc / 100

    # # 断言确保没有超出范围的值
    assert not np.any(ice_conc > 1)

    return ice_conc


def unfold_StackOverChannel(img, patch_size):
    """
    将图像切分成多个小块并沿着通道堆叠
    Args:
        img (N, *, C, H, W): 最后两个维度必须是空间维度
        patch_size(k_h,k_w): 长度为2的元组，就是configs.patch_size
    Returns:
        output (N, *, C*k_h*k_w, H/k_h, W/k_w)
    """
    n_dim = len(img.size())
    assert n_dim == 4 or n_dim == 5
    if patch_size[0] == 1 and patch_size[1] == 1:
        return img

    pt = img.unfold(-2, size=patch_size[0], step=patch_size[0])
    pt = pt.unfold(-2, size=patch_size[1], step=patch_size[1]).flatten(-2)
    if n_dim == 4:  # (N, C, H, W)
        pt = pt.permute(0, 1, 4, 2, 3).flatten(1, 2)
    elif n_dim == 5:  # (N, T, C, H, W)
        pt = pt.permute(0, 1, 2, 5, 3, 4).flatten(2, 3)
    assert pt.size(-3) == img.size(-3) * patch_size[0] * patch_size[1]
    return pt


def fold_tensor(tensor, output_size, patch_size):
    """
    用non-overlapping的块重建图像
    Args:
        input tensor shape (N, *, C*k_h*k_w, h, w)
        output_size: (H, W)，要重建的原始图像的大小
        patch_size: (k_h, k_w)
        请注意，对于non-overlapping的滑动窗口，通常stride等于patch_size
    Returns:
        output (N, *, C, H=n_h*k_h, W=n_w*k_w)
    """
    n_dim = len(tensor.shape)
    assert n_dim == 4 or n_dim == 5

    if patch_size[0] == 1 and patch_size[1] == 1:
        return tensor

    # 展平输入
    f = tensor.flatten(0, 1) if n_dim == 5 else tensor

    # 使用 F.fold 函数进行重建
    folded = F.fold(
        f.flatten(-2),
        output_size=output_size,
        kernel_size=patch_size,
        stride=patch_size,
    )

    if n_dim == 5:
        folded = folded.view(tensor.size(0), tensor.size(1), *folded.shape[1:])

    return folded


def prepare_inputs_targets(
    len_time, input_gap, input_length, pred_shift, pred_gap, pred_length, samples_gap
):
    """
    Args:
        input_gap: 两个连续输入帧之间的时间间隔
        input_length: 输入帧的数量
        pred_shift: 最后一个目标预测的前导时间
        pred_gap: 两个连续输出帧之间的时间间隔
        pred_length: 输出帧的数量
        samples_gap: 两个检索样本的起始时间之间的间隔
    Returns:
        idx_inputs: 指向输入样本位置的索引
        idx_targets: 指向目标样本位置的索引
    """
    assert pred_shift >= pred_length
    input_span = input_gap * (input_length - 1) + 1
    input_ind = np.arange(0, input_span, input_gap)
    target_ind = np.arange(0, pred_shift, pred_gap) + input_span + pred_gap - 1
    ind = np.concatenate([input_ind, target_ind]).reshape(1, input_length + pred_length)
    max_n_sample = len_time - (input_span + pred_shift - 1)
    ind = ind + np.arange(max_n_sample)[:, np.newaxis] @ np.ones(
        (1, input_length + pred_length), dtype=int
    )
    idx_inputs = ind[::samples_gap, :input_length]
    idx_targets = ind[::samples_gap, input_length:]
    assert len(idx_inputs) == len(idx_targets)
    return idx_inputs, idx_targets


class SIC_dataset(Dataset):
    def __init__(
        self,
        full_data_path,
        start_time,
        end_time,
        input_gap,
        input_length,
        pred_shift,
        pred_gap,
        pred_length,
        samples_gap,
    ):
        """
        Args:
            full_data_path: 可为单个 .nc 路径或多个 .nc 路径的列表；每个文件应包含变量 imgs，时间坐标与 GenTimeList 对齐
            start_time/end_time: 用于指定数据集（train/eval/test）的时间范围
            input_gap: 两个连续输入帧之间的时间间隔
            input_length: 输入帧的数量
            pred_shift: 最后一个目标预测的前导时间
            pred_gap: 两个连续输出帧之间的时间间隔
            pred_length: 输出帧的数量
            samples_gap: 两个检索样本的起始时间之间的间隔
        """
        super().__init__()

        self.times = GenTimeList(start_time, end_time)

        # 支持多文件按通道堆叠，或单文件单通道
        def _load_one_nc(path, target_times):
            # 读取单个 .nc 文件，按月份筛选时间并返回 [T, H, W]
            with xr.open_dataset(path, engine="netcdf4") as ds:
                # 选择变量名：优先使用 'imgs'，否则取第一个数据变量
                data_vars = list(ds.data_vars)
                var_name = "imgs" if "imgs" in data_vars else data_vars[0]

                # 处理时间坐标为 YYYY-MM 字符串，便于和 target_times 匹配
                ds_times = ds["time"].values  # numpy array of int64, e.g., [198801, ...]
                # 修复：直接格式化为 'YYYY-MM' 字符串（无需 datetime64）
                ds_months = np.array([f"{str(int(t))[:4]}-{str(int(t))[4:6]}" for t in ds_times])

                target_months = np.array([f"{str(t)[:4]}-{str(t)[4:6]}" for t in target_times])

                # 生成掩码并选择对应时间片
                mask = np.isin(ds_months, target_months)
                arr = ds[var_name].values  # [N_time, H, W]
                arr = arr[mask]
                return arr

        if isinstance(full_data_path, (list, tuple)):
            channel_arrays = []
            for p in full_data_path:
                arr = _load_one_nc(p, self.times)  # [T, H, W]
                channel_arrays.append(arr)
            # 堆叠为 [T, C, H, W]
            self.data = np.stack(channel_arrays, axis=1)
        else:
            arr = _load_one_nc(full_data_path, self.times)  # [T, H, W]
            # 单通道时扩展为 [T, 1, H, W]
            self.data = arr[:, None, ...]

        self.idx_inputs, self.idx_targets = prepare_inputs_targets(
            self.data.shape[0],
            input_gap=input_gap,
            input_length=input_length,
            pred_shift=pred_shift,
            pred_gap=pred_gap,
            pred_length=pred_length,
            samples_gap=samples_gap,
        )

    def __len__(self):
        return len(self.idx_inputs)

    def __getitem__(self, index):
        # 返回形状为 (T, C, H, W)
        return (
            self.data[self.idx_inputs[index]],
            self.data[self.idx_targets[index]],
        )

    def GetInputs(self):
        # 返回 (B, T, C, H, W)
        return self.data[self.idx_inputs]

    def GetTargets(self):
        # 返回 (B, T, C, H, W)
        return self.data[self.idx_targets]

    def GetTimes(self):
        return np.array(self.times)[
            np.concatenate([self.idx_inputs, self.idx_targets], axis=1)
        ]

    def GetDataSetShape(self):
        inputs_B = self.idx_inputs.shape[0]
        inputs_T, inputs_C, inputs_H, inputs_W = self.data[self.idx_inputs[0]].shape

        targets_B = self.idx_targets.shape[0]
        targets_T, targets_C, targets_H, targets_W = self.data[self.idx_targets[0]].shape

        return {
            "inputs(B, T, C, H, W)": (inputs_B, inputs_T, inputs_C, inputs_H, inputs_W),
            "targets(B, T, C, H, W)": (
                targets_B,
                targets_T,
                targets_C,
                targets_H,
                targets_W,
            ),
        }